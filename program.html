<!DOCTYPE HTML>
<!--
	Photon by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>LHMP 2021 workshop</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">

	<div id="wrapper">

		<!-- Header -->
		<section id="header" height="100px">
			<div class="inner">
				<!-- Nav -->
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>
				<span class="symbol"><img width="100px" src="images/lhmp-icon-2021-shadow.svg" alt="" /></span>
				<h2>Program</h2>
			</div>
		</section>

		<!-- Menu -->
		<nav id="menu">
			<h2>Menu</h2>
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="program.html">Program</a></li>
				<li><a href="call_for_papers.html">Call for Papers</a></li>
				<li><a href="challenge.html">Challenge</a></li>
				<li><a href="past_events.html">Past Events</a></li>
			</ul>
		</nav>


		<section id="one" class="main style1">

			<!-- One -->
			<div class="container">
				The program of this workshop includes 8 talks, spotlight presentations for the selected papers and a
				trajectory prediction challenge.

				In order to account for the diverse time zones of the invited speakers, the workshop will be split into
				the morning and the evening session in the Central European Summer Time zone (CEST). The talks will be
				recorded and all materials will be available on this website.</p>

				You can attend the workshop via this <a href="https://epfl.zoom.us/j/66418269303">link</a>
				<br>
				<h3>Morning session</h3>

				<div class="table-wrapper">
					<table>
						<thead>
							<tr>
								<th>Time CEST (PST)</th>
								<th>Speaker</th>
								<th>Topic</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>09:30 - 09:45 (00:30 - 00:45 AM)</td>
								<td>Organizers</td>
								<td>Welcome and Introduction</td>
							</tr>
							<tr>
								<td>09:45 - 10:15 (00:45 - 01:15 AM)</td>
								<td><strong><a href="https://www.professoren.tum.de/en/haddadin-sami">Sami
											Haddadin</a></strong>, TUM</td>
								<td>Safe Motion and Interaction in physical Human-Robot Interaction</td>
							</tr>
							<tr>
								<td colspan="3">Robots need to be able to safely and sensitively interact with humans
									and their environment. This long standing goal of robotics research is still
									considered a grand challenge. However, recently significant leaps forward in
									physical human-robot interaction were made, including human-safe motion planning
									based on injury biomechanics. In the latter, the principles of mechanics are applied
									to the analysis of human response and trauma in order to understand how injuries
									happen at the level of the bones, joints, organs, and tissues of the body.
									Quantifying the response at those levels leads one to define injury tolerances at
									which humans fail to recover. Increased knowledge about the specific underlying
									injury mechanisms, understanding of human pain and injury probability together with
									associated tolerances allow embedding those data-driven models into the motion and
									control paradigms of collaborative mobile robots and therefore achieve guaranteed
									safe motion planning at maximum performance. </td>
							</tr>
							<tr>
								<td>10:15 - 10:45 (01:15 - 01:45 AM)</td>
								<td><strong><a href="https://www.monash.edu/engineering/danakulic">Dana
											Kulic</a></strong>, Monash University</td>
								<td>Human motion prediction from demonstrations and interaction</td>
							</tr>
							<tr>
								<td colspan="3">Understanding human movement is an important topic in biomechanics,
									human-robot interaction and rehabilitation engineering. In this talk, I will first
									describe our recent work on estimating human objectives by observing human actions
									and preferences, including time-varying objectives and and from incomplete
									observations, and illustrate how estimates of human objectives can be used for
									behaviour prediction. Next, I will describe how models of human movement during
									interaction can be estimated in-the-loop and used to guide interaction policy.</td>
							</tr>
							<tr>
								<td>10:45 - 11:00 (01:45 - 02:00 AM)</td>
								<td>Break</td>
								<td></td>
							</tr>
							<tr>
								<td>11:00 - 11:30 (02:00 - 02:30 AM)</td>
								<td><strong><a href="https://www.kth.se/profile/lihuiw">Lihui Wang</a></strong>, KTH
								</td>
								<td>Motion Prediction for Human-Robot Collaborative Assembly </td>
							</tr>
							<tr>
								<td colspan="3">Human-robot collaboration has attracted increasing attentions in recent
									years, both in academia and in industry. For example, in human-robot collaborative
									assembly, robots are often required to dynamically change their pre-planned
									trajectories to collaborate with human operators in a shared workspace. However,
									industrial robots used today are controlled by pre-generated rigid codes that cannot
									support effective human-robot collaboration. In response to this need, human motion
									prediction is crucial for both collision avoidance and proactive assistance to
									humans, in addition to multi-modal robot control. Deep learning is used for
									classification, recognition and context awareness identification. Within the
									context, this presentation provides an overview as well as technical treatment on
									motion prediction for human-robot collaboration. Remaining challenges and future
									directions will also be highlighted. </td>
							</tr>
							<tr>
								<td>11:30 - 12:00 (02:30 - 03:00 AM)</td>
								<td><a href="challenge.html">TrajNet++ trajectory prediction challenge</a></td>
								<td></td>
							</tr>
							<tr>
								<td>12:00 - 12:10 (03:00 - 03:10 AM)</td>
								<td>Organizers</td>
								<td>Concluding the first session</td>
							</tr>
						</tbody>
					</table>
				</div>

				<h3>Evening session</h3>

				<div class="table-wrapper">
					<table>
						<thead>
							<tr>
								<th>Time CEST (PST)</th>
								<th>Speaker</th>
								<th>Topic</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>17:00 - 17:10 (08:00 - 08:10 AM)</td>
								<td>Organizers</td>
								<td>Introducing the second session</td>
							</tr>
							<tr>
								<td>17:10 - 17:40 (08:10 - 08:40 AM)</td>
								<td><strong><a href="https://www.hrl.uni-bonn.de/Members/maren">Maren
											Bennewitz</a></strong>, University of Bonn</td>
								<td>Anticipating Human Movements and Foresighted Robot Navigation Using Learned
									Human-Object Interactions</td>
							</tr>
							<tr>
								<td colspan="3">Service robots that help humans in their everyday life should avoid
									interferences with them and adapt their navigation behavior
									accordingly. This requires a robot to anticipate the user's movements
									and to infer where support will be needed. In this talk, I present an
									approach that predicts the user’s navigation goal based on the robot’s
									observations and prior knowledge about typical human transitions
									between objects. Our system then generates a navigation strategy that
									minimizes the robot’s arrival time at the navigation goal and, at the
									same time, complies with the user’s comfort during the movement.</td>
							</tr>
							<tr>
								<td>17:40 - 18:10 (08:40 - 09:10 AM)</td>
								<td><strong><a href="http://www.mit.edu/~jhow/">Jonathan P. How</a></strong>, MIT</td>
								<td>Context-aware learning of human motion prediction for safe autonomous driving</td>
							</tr>
							<tr>
								<td colspan="3">Understanding pedestrian behavior is crucial for safe autonomous
									navigation on the roads and in crowded environments. However, pedestrian motion is
									highly stochastic and involves various forms of interaction with the environment and
									other users of the road/sidewalk. While learning from large datasets is a promising
									strategy for developing predictive models, current methods are often limited to
									batch (offline) settings, thus limiting the ability for the model to generalize to
									new environments and update as new data becomes available. To address this
									challenge, this talk presents a Similarity-based Incremental Learning Algorithm
									(SILA) for pedestrian motion prediction. The talk will also discuss a new sharing
									pipeline called SimFuse, which enables autonomous agents to update their models by
									communicating with other vehicles (V2V) or infrastructure (V2I). Finally, the talk
									will describe our recent work toward certifying learned models to provide safety and
									robustness guarantees. This talk will conclude by highlighting some remaining open
									challenges in the field.
								</td>
							</tr>
							<tr>
								<td>18:10 - 18:20 (09:10 - 09:20 AM)</td>
								<td>Break</td>
								<td></td>
							</tr>
							<tr>
								<td>18:20 - 18:50 (09:20 - 09:50 AM)</td>
								<td><strong><a href="http://elenacorinagrigore.com/">Elena Corina Grigore</a></strong>,
									Motional</td>
								<td>Motion Forecasting for Autonomous Driving Applications</td>
							</tr>
							<tr>
								<td colspan="3">Autonomous driving requires operating in dynamic, interactive, and
									uncertain environments. Urban environments, in particular, present significant
									challenges for self-driving vehicles, which need to share the road with other agents
									such as vehicles, bicylists, and pedestrians. To do so, understanding the road
									context and making effective predictions of the behavior of these actors are
									crucial. Therefore, trajectory prediction is a core component of safe and confident
									operation. Among other difficulties, trajectory prediction is challenging due to the
									inherent multi-modality of this problem, with useful predictions needing to
									represent multiple possibilities and their associated likelihoods. In this talk, we
									present a solution for tackling multi-modality in a way that avoids mode collapse
									issues, and ensures we predict physically realizable trajectories. We do so by
									framing the trajectory prediction problem as classification over a diverse set of
									trajectory sets, and contribute three types of trajectory sets that can be employed
									in this context. Lastly, we also present a thorough comparison between our method
									and existing relevant work, contrasting regression, classification, and hybrid
									methods for trajectory prediction in self-driving.</td>
							</tr>
							<tr>
								<td>18:50 - 19:20 (09:50 - 10:20 AM)</td>
								<td><strong><a href="http://bensapp.github.io/">Benjamin Sapp</a></strong>, Waymo</td>
								<td>Long term prediction in complex interactive environments</td>
							</tr>
							<tr>
								<td colspan="3">Long-term human motion prediction is a critical component of scalable
									autonomous driving systems. There has been a multitude of core modeling improvements
									for this task in recent years, in large part fueled by popular public benchmarks.
									However, existing datasets, metrics, and output representations leave much to be
									desired in their ability to capture interactions between agents. In this talk, we go
									over recent advances in modeling, datasets, evaluation and output representations at
									Waymo, with a particular emphasis on modeling interactions.
								</td>
							</tr>
							<tr>
								<td>19:20 - 19:30 (10:20 - 10:30 AM)</td>
								<td>Break</td>
								<td></td>
							</tr>
							<tr>
								<td>19:30 - 20:00 (10:30 - 11:00 AM)</td>
								<td><strong><a href="https://people.eecs.berkeley.edu/~nrhinehart/">Nick
											Rhinehart</a></strong>, UC Berkeley</td>
								<td>Title</td>
							</tr>
							<tr>
								<td colspan="3">Abstract</td>
							</tr>
							<tr>
								<td>20:00 - 20:40 (11:00 - 11:40 AM)</td>
								<td><a href="call_for_papers.html">Paper spotlight presentations</a></td>
								<td></td>
							</tr>
							<tr>
								<td>20:40 - 20:50 (11:40 - 11:50 AM)</td>
								<td>Organizers</td>
								<td>Concluding the second session</td>
							</tr>
						</tbody>
					</table>
				</div>

				<h3>Accepted papers</h3>

				<table>
					<thead>
						<tr>
							<th>Authors</th>
							<th>Paper</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Wenjie Yin, Hang Yin, Danica Kragic Jensfelt and Mårten Björkman</td>
							<td>Long-term Human Motion Generation and Reconstruction Using Graph-based Normalizing Flow
								<a href="papers/LHMP_2021_paper_yin.pdf">[paper]</a>
							</td>
						</tr>
						<tr>
							<td>Lingfeng Sun, Masayoshi Tomizuka and Wei Zhan</td>
							<td>Multi-Style Human Motion Prediction and Generation via Meta-Learning <a
									href="papers/LHMP_2021_paper_sun.pdf">[paper]</a></td>
						</tr>
						<tr>
							<td>Bronwyn Biro, Zhitian Zhang, Mo Chen and Angelica Lim</td>
							<td>Pose Forecasting in the SFU-Store-Nav 3D Virtual Human Platform <a
									href="papers/LHMP_2021_paper_biro.pdf">[paper]</a></td>
						</tr>
						<tr>
							<td>An Le, Philipp Kratzer, Simon Hagenmayer, Marc Toussaint and Jim Mainprice</td>
							<td>Hierarchical Prediction and Planning for Human-Robot Collaboration <a
									href="papers/LHMP_2021_paper_le.pdf">[paper]</a></td>
						</tr>
					</tbody>
				</table>

				<ul class="actions">
					<li><a href="index.html" class="button">Back</a></li>
				</ul>
			</div>

		</section>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>